{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 | TKO_2096 Application of Data Analysis 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested cross-validation for K-nearest neighbors <br>\n",
    "- Use Python 3 to program a nested cross-validation for the k-nearest neighbors (kNN) method so that the number of neighbours k is automatically selected from the range 1 to 10. In other words, the base learning algorithm is kNN but the actual learning algorithm, whose prediction performance will be evaluated with nested CV, is kNN with automatic CV-based model selection (see the lectures and the pseudo codes presented on them for more info on this interpretation).\n",
    "- As a kNN implementation, you can use sklearn: http://scikit-learn.org/stable/modules/neighbors.html but your own kNN implementation can also be used if you like to keep more control on what is happening in the learning process. The CV implementation should be easily modifiable, since the forthcoming exercises involve different problem-dependent CV variations.\n",
    "- Use the nested CV implementation on the iris data and report the resulting classification accuracy. Hint: you can use the nested CV example provided on sklearn documentation: https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html as a starting point and compare your nested CV implementation with that but do NOT use the ready made CV implementations of sklearn as the idea of the exercise is to learn to split the data on your own. The other exercises need more sophisticated data splitting which are not necessarily available in libraries.\n",
    "- Return your solution for each exercise BOTH as a Jupyter Notebook file and as a PDF-file made from it.\n",
    "- Return the report to the course page on **Monday 1st of February** at the latest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports: \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the nested cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting the best k value:\n",
      "Best k: 5, with score: 0.975\n",
      "Best k: 8, with score: 0.967\n",
      "Best k: 9, with score: 0.967\n",
      "Best k: 8, with score: 0.975\n",
      "Best k: 3, with score: 0.983\n",
      "\n",
      "Evaluating prediction performance of the selected model:\n",
      "Run #0: 0.967\n",
      "Run #1: 0.967\n",
      "Run #2: 0.933\n",
      "Run #3: 0.8\n",
      "Run #4: 0.833\n",
      "\n",
      "Mean score for outer cv was: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Parameters for cv and knn\n",
    "folds = 5\n",
    "n_neighbors = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "\n",
    "# K-fold split generators\n",
    "inner_cv = KFold(n_splits=folds, shuffle=True, random_state=44)\n",
    "outer_cv = KFold(n_splits=folds, shuffle=True, random_state=44)\n",
    "\n",
    "outer_scores = []\n",
    "\n",
    "print(\"Selecting the best k value:\")\n",
    "# Split the data into training and test sets for the outer cross-validation\n",
    "for train_out, test_out in outer_cv.split(X):\n",
    "    X_train, X_test = X[train_out], X[test_out]\n",
    "    y_train, y_test = y[train_out], y[test_out]\n",
    "    \n",
    "    scores = [] # Mean score for each value of k (10 values)\n",
    "    \n",
    "    # For every value of k...\n",
    "    for k in n_neighbors:\n",
    "        inner_scores = []\n",
    "        \n",
    "        # ...split the outer training set further into inner training and validation sets...\n",
    "        for train_in, validation in inner_cv.split(X_train):\n",
    "            X_train_inner, X_val = X_train[train_in], X_train[validation]\n",
    "            y_train_inner, y_val = y_train[train_in], y_train[validation]\n",
    "\n",
    "            # ...and do classification\n",
    "            knn = KNeighborsClassifier(k)\n",
    "            knn.fit(X_train_inner, y_train_inner)\n",
    "\n",
    "            # Score the model\n",
    "            score = knn.score(X_val, y_val)\n",
    "            inner_scores.append(score) # save score\n",
    "        \n",
    "        # Take mean score for every round of k-values\n",
    "        scores.append(np.mean(inner_scores))\n",
    "            \n",
    "    # Save this round's best score and k\n",
    "    best_score = max(scores)\n",
    "\n",
    "    best_k_for_this_round = scores.index(best_score)+1 # add 1 for indexing\n",
    "    \n",
    "    print(f\"Best k: {best_k_for_this_round}, with score: {round(best_score,3)}\")\n",
    "    \n",
    "    # Use best_k_for_this_round as number of neighbors in the outer cross-validation\n",
    "    knn = KNeighborsClassifier(best_k_for_this_round)\n",
    "    outer_score = cross_val_score(knn, X_test, y_test, cv=outer_cv)\n",
    "    outer_scores.append(np.mean(outer_score))\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Evaluating prediction performance of the selected model:\")\n",
    "\n",
    "for i, j in enumerate(outer_scores):\n",
    "    print(f\"Run #{i}: {round(j,3)}\")\n",
    "\n",
    "print()\n",
    "print(\"Mean score for outer cv was:\", round(np.mean(outer_scores),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD VERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "###### THIS IS THE OLD VERSION ######\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "# Do some definitions\n",
    "folds = 5\n",
    "n_neighbors = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "inner_cv = KFold(n_splits=folds, shuffle=True, random_state=44)\n",
    "outer_cv = KFold(n_splits=folds, shuffle=True, random_state=44)\n",
    "\n",
    "outer_scores = []\n",
    "\n",
    "\n",
    "# Split the data into training and test sets for the outer cross-validation\n",
    "for train_i, test_i in outer_cv.split(X):\n",
    "    X_train, X_test = X[train_i], X[test_i]\n",
    "    y_train, y_test = y[train_i], y[test_i]\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "\n",
    "    # Split the training set further into inner training and validation sets\n",
    "    for train_inner, valid in inner_cv.split(X_train):\n",
    "        X_train_inner, X_val = X_train[train_inner], X_train[valid]\n",
    "        y_train_inner, y_val = y_train[train_inner], y_train[valid]\n",
    "        \n",
    "        inner_scores = []\n",
    "        \n",
    "        # Get prediction scores for every value of k\n",
    "        for k in n_neighbors:\n",
    "            knn = KNeighborsClassifier(k)\n",
    "            knn.fit(X_train_inner, y_train_inner)\n",
    "            \n",
    "            score = knn.score(X_val, y_val)\n",
    "            inner_scores.append((k, score))\n",
    "            \n",
    "        # For every value of k, save the best score\n",
    "        scores.append(max(inner_scores))\n",
    "    \n",
    "    # Get the value of the best k\n",
    "    best_k = max(scores)[0]\n",
    "    \n",
    "    # And use it to run the outer cross-validation\n",
    "    knn = KNeighborsClassifier(best_k)\n",
    "    outer_score = cross_val_score(knn, X_test, y_test, cv=outer_cv)\n",
    "    outer_scores.append(np.mean(outer_score))\n",
    "    \n",
    "print(\"Accuracy:\", round(np.mean(outer_scores),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
